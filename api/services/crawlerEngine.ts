import Parser from 'rss-parser';
import { detectAssetType } from '../../src/lib/assetTypeMapping';
import { translateContent } from './AIService';
import getSupabaseClient from '../supabaseClient';

const parser = new Parser();

export class CrawlerEngine {
  /**
   * åŸ·è¡Œä¸€æ¬¡å®Œæ•´çš„çˆ¬èŸ²ä»»å‹™
   */
  static async run() {
    console.log('ğŸ•·ï¸ Crawler Engine Started...');
    const supabase = getSupabaseClient();
    
    // 1. ç²å–æ‰€æœ‰æ´»èºçš„ RSS Feeds
    const { data: feeds, error: feedsError } = await supabase
      .from('rss_feeds')
      .select('*')
      .eq('is_active', true);

    if (feedsError) throw feedsError;

    const results = [];

    // 2. éæ­·æ¯å€‹ Feed é€²è¡ŒæŠ“å–
    for (const feed of feeds || []) {
      try {
        const feedContent = await parser.parseURL(feed.url);
        let newItemsCount = 0;

        // 3. è™•ç†æ¯å€‹é …ç›®
        for (const item of feedContent.items) {
          if (!item.link) continue;

          // æª¢æŸ¥æ˜¯å¦å·²å­˜åœ¨
          const { data: existing } = await supabase
            .from('assets')
            .select('id')
            .eq('source_url', item.link)
            .single();

          if (!existing) {
            // è‡ªå‹•åˆ†é¡
            const assetType = detectAssetType(item.title + ' ' + (item.contentSnippet || ''));
            
            // ç¿»è­¯æ¨™é¡Œèˆ‡å…§å®¹ (Translation)
            const translatedTitle = await translateContent(item.title || '');
            const translatedDescription = await translateContent(item.contentSnippet || item.content || '');
            // const translatedTitle = item.title || ''; // Fallback
            // const translatedDescription = item.contentSnippet || item.content || ''; // Fallback

            // å¯«å…¥ assets è¡¨
            await supabase.from('assets').insert({
              asset_type: assetType,
              source_platform: 'rss',
              source_url: item.link,
              title: translatedTitle, // å„²å­˜ç¹é«”ä¸­æ–‡æ¨™é¡Œ
              description: translatedDescription, // å„²å­˜ç¹é«”ä¸­æ–‡æ‘˜è¦
              raw_content: item.content || item.contentSnippet, // ä¿ç•™åŸå§‹å…§å®¹
              processed_content: translatedDescription, // é è™•ç†å…§å®¹ä¹Ÿä½¿ç”¨ç¿»è­¯å¾Œçš„æ‘˜è¦
              category: feed.category || 'general',
              sub_category: item.categories?.[0] || '', 
              status: 'new',
              created_at: new Date(item.pubDate || new Date()).toISOString(),
              risk_level: 0, 
            });
            newItemsCount++;
          }
        }

        // æ›´æ–° Feed çš„æœ€å¾ŒæŠ“å–æ™‚é–“
        await supabase
          .from('rss_feeds')
          .update({ last_fetched_at: new Date().toISOString() })
          .eq('id', feed.id);

        // è¨˜éŒ„ Log
        await supabase.from('crawler_logs').insert({
          feed_id: feed.id,
          status: 'success',
          items_fetched: newItemsCount
        });

        results.push({ feed: feed.name, new_items: newItemsCount, status: 'success' });

      } catch (feedError: any) {
        console.error(`Failed to fetch feed ${feed.url}:`, feedError);
        
        await supabase.from('crawler_logs').insert({
          feed_id: feed.id,
          status: 'failed',
          error_message: feedError.message
        });

        results.push({ feed: feed.name, status: 'failed', error: feedError.message });
      }
    }

    console.log('ğŸ•·ï¸ Crawler Engine Finished:', results);
    return results;
  }

  /**
   * æ–°å¢ RSS Feed
   */
  static async addFeed(name: string, url: string, category: string) {
    const supabase = getSupabaseClient();
    const { data, error } = await supabase
      .from('rss_feeds')
      .insert({ name, url, category })
      .select()
      .single();

    if (error) throw error;
    return data;
  }
}
